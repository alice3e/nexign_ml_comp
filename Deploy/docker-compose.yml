version: '3.8'

services:
  # База данных SQLite
  database:
    build:
      context: ./DB
      dockerfile: Dockerfile
    container_name: database
    ports:
      - "8003:8003"
    volumes:
      - ./DB/docker-volumes/sqlite-db:/data
    environment:
      - DB_PATH=/data/requests.db
    networks:
      - diagram-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8003/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Adapter Service - конвертация форматов
  adapter:
    build:
      context: ./adapter
      dockerfile: Dockerfile
    container_name: adapter
    ports:
      - "8001:8001"
    networks:
      - diagram-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8001/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # VLM Inference Service - распознавание диаграмм
  vlm-inference:
    build:
      context: ./ML-container
      dockerfile: Dockerfile
    container_name: vlm-inference
    ports:
      - "8002:8002"
    volumes:
      # Веса LoRA адаптеров (read-only)
      - ./ML-container/docker-volumes/weights:/app/models/weights:ro
      # Кэш HuggingFace для базовой модели
      - ./ML-container/docker-volumes/base-model:/root/.cache/huggingface:ro
    environment:
      - BASE_MODEL_ID=${BASE_MODEL_ID:-Qwen/Qwen3-VL-2B-Instruct}
      - ADAPTER_PATH=/app/models/weights
      - DEVICE=${DEVICE:-cpu}
      - TORCH_DTYPE=${TORCH_DTYPE:-float16}
      - MAX_NEW_TOKENS=${MAX_NEW_TOKENS:-384}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - diagram-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8002/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Модель долго загружается
    restart: unless-stopped

  # Backend API - координация сервисов
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    ports:
      - "8000:8000"
    environment:
      - VLM_SERVICE_URL=http://vlm-inference:8002
      - ADAPTER_SERVICE_URL=http://adapter:8001
      - DB_SERVICE_URL=http://database:8003
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-120}
    depends_on:
      - database
      - adapter
      - vlm-inference
    networks:
      - diagram-network
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health').raise_for_status()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Frontend UI - Streamlit интерфейс
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    ports:
      - "8501:8501"
    environment:
      - BACKEND_URL=http://backend:8000
    depends_on:
      - backend
    networks:
      - diagram-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

networks:
  diagram-network:
    driver: bridge

# Volumes для персистентности данных
# Примечание: volumes для ML-container и DB монтируются напрямую из хоста