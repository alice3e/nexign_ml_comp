"""
VLM Inference Service - FastAPI —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Qwen3-VL-2B-Instruct —Å –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
"""

import os
import time
import logging
from typing import Optional
from contextlib import asynccontextmanager

import torch
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from PIL import Image
from io import BytesIO

from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
from qwen_vl_utils import process_vision_info

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
BASE_MODEL_ID = os.getenv("BASE_MODEL_ID", "Qwen/Qwen3-VL-2B-Instruct")
ADAPTER_PATH = os.getenv("ADAPTER_PATH", "/app/models/weights")
DEVICE = os.getenv("DEVICE", "cpu")
TORCH_DTYPE = os.getenv("TORCH_DTYPE", "float16")
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "384"))
#os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
processor = None
model_load_time = None

# –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
SYSTEM_PROMPT = (
    "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ BPMN. –í—ã–¥–∞–≤–∞–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown-—Ç–∞–±–ª–∏—Ü—ã. "
    "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ—á–Ω–æ: | ‚Ññ | –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è | –†–æ–ª—å |."
)

# –ú–µ—Ç—Ä–∏–∫–∏
inference_count = 0
total_inference_time = 0.0


def load_model_and_processor():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞.
    –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
    """
    global model, processor, model_load_time
    
    logger.info("=" * 60)
    logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VLM Inference Service")
    logger.info("=" * 60)
    logger.info(f"üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {BASE_MODEL_ID}")
    logger.info(f"üîß –ê–¥–∞–ø—Ç–µ—Ä—ã: {ADAPTER_PATH}")
    logger.info(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    logger.info(f"üî¢ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {TORCH_DTYPE}")
    logger.info("=" * 60)
    
    start_time = time.time()
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if DEVICE == "cuda" and torch.cuda.is_available():
            device = torch.device("cuda")
            logger.info("‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU")
        elif DEVICE == "mps" and torch.backends.mps.is_available():
            device = torch.device("mps")
            logger.info("‚úÖ MPS –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º Apple Silicon GPU")
        else:
            device = torch.device("cpu")
            logger.info("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ)")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dtype
        dtype = torch.float16 if TORCH_DTYPE == "float16" else torch.float32
        logger.info(f"üî¢ –ò—Å–ø–æ–ª—å–∑—É–µ–º dtype: {dtype}")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        model = Qwen3VLForConditionalGeneration.from_pretrained(
            BASE_MODEL_ID,
            torch_dtype=dtype,
            device_map="auto" if DEVICE == "cuda" else None
        )
        
        if DEVICE != "cuda":
            model = model.to(device)
        
        logger.info("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...")
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞ (–µ—Å–ª–∏ —Ç–∞–º –µ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥)
            processor = AutoProcessor.from_pretrained(
                ADAPTER_PATH,
                min_pixels=256*28*28,
                max_pixels=512*28*28
            )
            logger.info("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞: {e}")
            processor = AutoProcessor.from_pretrained(
                BASE_MODEL_ID,
                min_pixels=256*28*28,
                max_pixels=512*28*28
            )
            logger.info("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
        
        # 3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤
        if os.path.exists(ADAPTER_PATH):
            logger.info("‚è≥ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
            try:
                model = PeftModel.from_pretrained(model, ADAPTER_PATH)
                model.eval()
                logger.info("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤: {e}")
                logger.warning("‚ö†Ô∏è  –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é")
        else:
            logger.warning(f"‚ö†Ô∏è  –ê–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {ADAPTER_PATH}")
            logger.warning("‚ö†Ô∏è  –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è")
        
        model_load_time = time.time() - start_time
        
        logger.info("=" * 60)
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞ {model_load_time:.2f} —Å–µ–∫")
        logger.info("=" * 60)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / (1024**3)
            memory_reserved = torch.cuda.memory_reserved() / (1024**3)
            logger.info(f"üíæ GPU –ø–∞–º—è—Ç—å: {memory_allocated:.2f} GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f} GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        logger.error("=" * 60)
        raise


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    # Startup
    logger.info("üîÑ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞...")
    load_model_and_processor()
    logger.info("‚úÖ –°–µ—Ä–≤–∏—Å –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    
    yield
    
    # Shutdown
    logger.info("üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–∏—Å–∞...")
    logger.info(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {inference_count}")
    if inference_count > 0:
        avg_time = total_inference_time / inference_count
        logger.info(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {avg_time:.2f} —Å–µ–∫")


# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="VLM Inference Service",
    description="–°–µ—Ä–≤–∏—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qwen3-VL",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {
        "status": "healthy" if model is not None else "initializing",
        "model_loaded": model is not None,
        "processor_loaded": processor is not None,
        "device": DEVICE,
        "model_load_time": model_load_time
    }


@app.get("/metrics")
async def get_metrics():
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ä–∞–±–æ—Ç—ã —Å–µ—Ä–≤–∏—Å–∞
    """
    avg_inference_time = (
        total_inference_time / inference_count if inference_count > 0 else 0
    )
    
    metrics = {
        "inference_count": inference_count,
        "total_inference_time": round(total_inference_time, 2),
        "avg_inference_time": round(avg_inference_time, 2),
        "model_load_time": round(model_load_time, 2) if model_load_time else None,
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
    if torch.cuda.is_available():
        metrics["gpu_memory_allocated_gb"] = round(
            torch.cuda.memory_allocated() / (1024**3), 2
        )
        metrics["gpu_memory_reserved_gb"] = round(
            torch.cuda.memory_reserved() / (1024**3), 2
        )
    
    return metrics


@app.post("/infer")
async def infer(file: UploadFile = File(...)):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    
    Args:
        file: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã (PNG, JPG, JPEG)
    
    Returns:
        JSON —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    """
    global inference_count, total_inference_time
    
    if model is None or processor is None:
        logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        raise HTTPException(
            status_code=503,
            detail="–ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ"
        )
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if not file.content_type.startswith("image/"):
        logger.warning(f"‚ö†Ô∏è  –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file.content_type}")
        raise HTTPException(
            status_code=400,
            detail=f"–û–∂–∏–¥–∞–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –ø–æ–ª—É—á–µ–Ω–æ: {file.content_type}"
        )
    
    logger.info("=" * 60)
    logger.info(f"üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å")
    logger.info(f"üìÑ –§–∞–π–ª: {file.filename}")
    logger.info(f"üì¶ –¢–∏–ø: {file.content_type}")
    
    try:
        # –ß—Ç–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        start_time = time.time()
        
        contents = await file.read()
        image = Image.open(BytesIO(contents)).convert("RGB")
        
        logger.info(f"üñºÔ∏è  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.size}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": SYSTEM_PROMPT}
            ]
        }]
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ chat template
        text_input = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ vision inputs
        image_inputs, video_inputs = process_vision_info(messages)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs = processor(
            text=[text_input],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt"
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device = next(model.parameters()).device
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                  for k, v in inputs.items()}
        
        logger.info("‚è≥ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        generation_start = time.time()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.inference_mode():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=False
            )
        
        generation_time = time.time() - generation_start
        logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {generation_time:.2f} —Å–µ–∫")
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        total_time = time.time() - start_time
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        inference_count += 1
        total_inference_time += total_time
        
        logger.info(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
        logger.info(f"üìä –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(output_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.info("=" * 60)
        
        return JSONResponse(
            content={
                "description": output_text,
                "metadata": {
                    "inference_time": round(total_time, 2),
                    "generation_time": round(generation_time, 2),
                    "image_size": list(image.size),
                    "model": BASE_MODEL_ID,
                    "device": str(device)
                }
            }
        )
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ: {e}")
        logger.error("=" * 60)
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}"
        )


@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–µ—Ä–≤–∏—Å–µ"""
    return {
        "service": "VLM Inference Service",
        "version": "1.0.0",
        "model": BASE_MODEL_ID,
        "status": "ready" if model is not None else "initializing",
        "endpoints": {
            "health": "/health",
            "metrics": "/metrics",
            "infer": "/infer (POST)",
            "docs": "/docs"
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)