# VLM Inference Service

–ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –¥–∏–∞–≥—Ä–∞–º–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qwen3-VL-2B-Instruct —Å –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞

```
ML-container/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt     # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ docker-volumes/
‚îÇ   ‚îú‚îÄ‚îÄ base-model/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ download_qwen3.py  # –°–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
‚îÇ   ‚îî‚îÄ‚îÄ weights/             # LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã (–º–æ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä)
‚îÇ       ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ       ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ       ‚îú‚îÄ‚îÄ chat_template.jinja
‚îÇ       ‚îú‚îÄ‚îÄ params.json
‚îÇ       ‚îú‚îÄ‚îÄ processor_config.json
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ       ‚îî‚îÄ‚îÄ tokenizer.json
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .dockerignore
‚îî‚îÄ‚îÄ README.md
```

## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∑–∞–ø—É—Å–∫—É

### 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏

–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Qwen3-VL-2B-Instruct (~4-5 GB) –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –∏–∑ HuggingFace Hub. –ú–æ–¥–µ–ª—å –∫—ç—à–∏—Ä—É–µ—Ç—Å—è –≤ Docker volume –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å:

```bash
cd docker-volumes/base-model
pip install huggingface-hub
python download_qwen3.py
```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤

–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ `docker-volumes/weights/` –Ω–∞—Ö–æ–¥—è—Ç—Å—è —Ñ–∞–π–ª—ã –∞–¥–∞–ø—Ç–µ—Ä–æ–≤:
- `adapter_config.json`
- `adapter_model.safetensors`
- `processor_config.json`
- `tokenizer_config.json`
- –∏ –¥—Ä—É–≥–∏–µ

## –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–∞

```bash
docker build -t vlm-inference:latest .
```

## –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞

–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:
- –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω `DEVICE=cuda` –∏ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU
- –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω `DEVICE=mps` –∏ MPS –¥–æ—Å—Ç—É–ø–Ω–∞ (Mac M1/M2/M3) ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Apple Silicon
- –ï—Å–ª–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ –∏–ª–∏ —É–∫–∞–∑–∞–Ω `DEVICE=cpu` ‚Üí fallback –Ω–∞ CPU

**–í–∞–∂–Ω–æ:** –î–ª—è GPU –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å `--gpus all`, –∏–Ω–∞—á–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –Ω–µ —É–≤–∏–¥–∏—Ç GPU!

### CPU —Ä–µ–∂–∏–º (fallback, —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∑–¥–µ)

```bash
docker run -d \
  --name vlm-inference \
  -p 8002:8002 \
  -v $(pwd)/docker-volumes/weights:/app/models/weights:ro \
  -v vlm-cache:/root/.cache/huggingface \
  -e DEVICE=cpu \
  -e TORCH_DTYPE=float16 \
  vlm-inference:latest
```

### GPU —Ä–µ–∂–∏–º (NVIDIA CUDA, –ø—Ä–æ–¥–∞–∫—à–Ω)

```bash
docker run -d \
  --name vlm-inference \
  --gpus all \
  -p 8002:8002 \
  -v $(pwd)/docker-volumes/weights:/app/models/weights:ro \
  -v vlm-cache:/root/.cache/huggingface \
  -e DEVICE=cuda \
  -e TORCH_DTYPE=float16 \
  vlm-inference:latest
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –¢—Ä–µ–±—É–µ—Ç—Å—è NVIDIA Docker runtime. –£—Å—Ç–∞–Ω–æ–≤–∫–∞:
```bash
# Ubuntu/Debian
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Apple Silicon (MPS, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Mac)

```bash
docker run -d \
  --name vlm-inference \
  -p 8002:8002 \
  -v $(pwd)/docker-volumes/weights:/app/models/weights:ro \
  -v vlm-cache:/root/.cache/huggingface \
  -e DEVICE=mps \
  -e TORCH_DTYPE=float16 \
  vlm-inference:latest
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –¢—Ä–µ–±—É–µ—Ç—Å—è Docker Desktop –¥–ª—è Mac —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MPS (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è).

## –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | –û–ø–∏—Å–∞–Ω–∏–µ | –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é |
|------------|----------|----------------------|
| `BASE_MODEL_ID` | ID –º–æ–¥–µ–ª–∏ –Ω–∞ HuggingFace | `Qwen/Qwen3-VL-2B-Instruct` |
| `ADAPTER_PATH` | –ü—É—Ç—å –∫ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞–º | `/app/models/weights` |
| `DEVICE` | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (`cpu`/`cuda`/`mps`) | `cpu` |
| `TORCH_DTYPE` | –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö PyTorch (`float16`/`float32`) | `float16` |
| `MAX_NEW_TOKENS` | –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ | `384` |
| `HF_HOME` | –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫—ç—à–∞ HuggingFace | `/root/.cache/huggingface` |

## API Endpoints

### POST /infer

–í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –¥–∏–∞–≥—Ä–∞–º–º—ã.

**Request:**
```bash
curl -X POST "http://localhost:8002/infer" \
  -F "file=@diagram.png"
```

**Response:**
```json
{
  "description": "| ‚Ññ | –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è | –†–æ–ª—å |\n|---|---|---|\n| 1 | –ù–∞—á–∞–ª–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ | –°–∏—Å—Ç–µ–º–∞ |",
  "metadata": {
    "inference_time": 7.52,
    "generation_time": 6.89,
    "image_size": [1024, 768],
    "model": "Qwen/Qwen3-VL-2B-Instruct",
    "device": "cpu"
  }
}
```

### GET /health

–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞.

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "processor_loaded": true,
  "device": "cpu",
  "model_load_time": 45.23
}
```

### GET /metrics

–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ä–∞–±–æ—Ç—ã —Å–µ—Ä–≤–∏—Å–∞.

**Response:**
```json
{
  "inference_count": 42,
  "total_inference_time": 315.84,
  "avg_inference_time": 7.52,
  "model_load_time": 45.23,
  "gpu_memory_allocated_gb": 5.8,
  "gpu_memory_reserved_gb": 6.2
}
```

### GET /

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–µ—Ä–≤–∏—Å–µ.

### GET /docs

Swagger UI –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è FastAPI).

## –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

–°–µ—Ä–≤–∏—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—Ä–æ–≤–Ω—è–º–∏:
- **INFO** - –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞, –º–µ—Ç—Ä–∏–∫–∏
- **WARNING** - –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã)
- **ERROR** - –æ—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–æ–≤

–ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤:
```bash
docker logs -f vlm-inference
```

–ü—Ä–∏–º–µ—Ä –ª–æ–≥–æ–≤ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ:
```
============================================================
üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VLM Inference Service
============================================================
üì¶ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: Qwen/Qwen3-VL-2B-Instruct
üîß –ê–¥–∞–ø—Ç–µ—Ä—ã: /app/models/weights
üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda
üî¢ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: float16
============================================================
‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º GPU
üî¢ –ò—Å–ø–æ–ª—å–∑—É–µ–º dtype: torch.float16
‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...
‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞...
‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∞–¥–∞–ø—Ç–µ—Ä–∞
‚è≥ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...
‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã
============================================================
‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞ 45.23 —Å–µ–∫
============================================================
üíæ GPU –ø–∞–º—è—Ç—å: 5.8 GB –≤—ã–¥–µ–ª–µ–Ω–æ, 6.2 GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ
```

–ü—Ä–∏–º–µ—Ä –ª–æ–≥–æ–≤ –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ:
```
============================================================
üì• –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
üìÑ –§–∞–π–ª: diagram.png
üì¶ –¢–∏–ø: image/png
üñºÔ∏è  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: (1024, 768)
‚è≥ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...
‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ 6.89 —Å–µ–∫
‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ
‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: 7.52 —Å–µ–∫
üìä –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: 245 —Å–∏–º–≤–æ–ª–æ–≤
============================================================
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º

| –†–µ–∂–∏–º | RAM | VRAM | –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ |
|-------|-----|------|-----------------|
| CPU (float16) | ~8 GB | - | 15-20 —Å–µ–∫ |
| GPU (float16) | ~4 GB | ~6 GB | 5-10 —Å–µ–∫ |
| MPS (float16) | ~6 GB | - | 8-12 —Å–µ–∫ |
| GPU (int8 –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ) | ~3 GB | ~4 GB | 7-12 —Å–µ–∫ |

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–î–ª—è CPU:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `TORCH_DTYPE=float16` –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
- –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–æ int8 —á–µ—Ä–µ–∑ `bitsandbytes`
- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `OMP_NUM_THREADS` –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —è–¥–µ—Ä

**–î–ª—è GPU:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `DEVICE=cuda` –∏ `--gpus all`
- –î–ª—è GPU —Å –ø–∞–º—è—Ç—å—é <8GB —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `torch.compile()` –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (—Ç—Ä–µ–±—É–µ—Ç PyTorch 2.0+)

**–î–ª—è Apple Silicon:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `DEVICE=mps`
- MPS —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ CPU, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ CUDA
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π MPS

## Troubleshooting

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è

**–ü—Ä–æ–±–ª–µ–º–∞:** –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace Hub.

**–†–µ—à–µ–Ω–∏–µ:**
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
2. –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ (~5 GB)
3. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø—Ä–∏–≤–∞—Ç–Ω–∞—è, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `HF_TOKEN`:
   ```bash
   docker run -e HF_TOKEN=your_token ...
   ```

### GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω

**–ü—Ä–æ–±–ª–µ–º–∞:** –£–∫–∞–∑–∞–Ω `DEVICE=cuda`, –Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.

**–†–µ—à–µ–Ω–∏–µ:**
1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–æ–±–∞–≤–∏–ª–∏ `--gpus all` –≤ docker run
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ NVIDIA Docker runtime —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω:
   ```bash
   docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
   ```
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA –Ω–∞ —Ö–æ—Å—Ç–µ:
   ```bash
   nvidia-smi
   ```

### Out of Memory (OOM)

**–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏.

**–†–µ—à–µ–Ω–∏–µ:**
1. –£–º–µ–Ω—å—à–∏—Ç–µ `MAX_NEW_TOKENS`
2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ (int8)
3. –£–≤–µ–ª–∏—á—å—Ç–µ –ª–∏–º–∏—Ç—ã –ø–∞–º—è—Ç–∏ Docker:
   ```bash
   docker run --memory=10g ...
   ```

### –ú–µ–¥–ª–µ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å

**–ü—Ä–æ–±–ª–µ–º–∞:** –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–Ω–∏–º–∞–µ—Ç >20 —Å–µ–∫—É–Ω–¥.

**–†–µ—à–µ–Ω–∏–µ:**
1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –≤–º–µ—Å—Ç–æ CPU (`DEVICE=cuda` + `--gpus all`)
2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `float16` –∞ –Ω–µ `float32`
3. –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≥—Ä—É–∑–∫—É —Å–∏—Å—Ç–µ–º—ã (`htop`, `nvidia-smi`)

### LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è

**–ü—Ä–æ–±–ª–µ–º–∞:** –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ "–ê–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã".

**–†–µ—à–µ–Ω–∏–µ:**
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ volume –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω:
   ```bash
   docker exec vlm-inference ls -la /app/models/weights
   ```
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª—ã –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥

### Healthcheck

Docker –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–¥–æ—Ä–æ–≤—å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥:

```bash
docker ps  # –°–º–æ—Ç—Ä–∏–º —Å—Ç–∞—Ç—É—Å –≤ –∫–æ–ª–æ–Ω–∫–µ STATUS
```

### –ú–µ—Ç—Ä–∏–∫–∏

–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —á–µ—Ä–µ–∑ API:
```bash
curl http://localhost:8002/metrics
```

### Prometheus (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

–î–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Prometheus –¥–æ–±–∞–≤—å—Ç–µ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä –º–µ—Ç—Ä–∏–∫ (TODO).

## –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–æ–µ

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫ —Å–µ—Ä–≤–∏—Å–∞:

```bash
docker run -d --name vlm-inference-1 --gpus '"device=0"' -p 8002:8002 ...
docker run -d --name vlm-inference-2 --gpus '"device=1"' -p 8003:8002 ...
docker run -d --name vlm-inference-3 --gpus '"device=2"' -p 8004:8002 ...
```

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –Ω–∞–≥—Ä—É–∑–∫–∏ (nginx, traefik) –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.

### –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–æ–µ

–£–≤–µ–ª–∏—á—å—Ç–µ —Ä–µ—Å—É—Ä—Å—ã –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞:

```bash
docker run -d \
  --cpus=8 \
  --memory=16g \
  --gpus all \
  ...
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å docker-compose

–°–º. –∫–æ—Ä–Ω–µ–≤–æ–π `docker-compose.yml` –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –¥—Ä—É–≥–∏–º–∏ —Å–µ—Ä–≤–∏—Å–∞–º–∏.