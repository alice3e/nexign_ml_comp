"""
Database module –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç SQLite –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏ –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
"""

import os
import sqlite3
from datetime import datetime
from typing import Optional, Dict, Any, List
from contextlib import contextmanager
import json
import logging

logger = logging.getLogger(__name__)

# –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DB_PATH = os.getenv("DB_PATH", "/app/data/requests.db")


def init_database():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    """
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ë–î –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS inference_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            
            -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø—Ä–æ—Å–µ
            request_timestamp TEXT NOT NULL,
            file_name TEXT NOT NULL,
            file_type TEXT NOT NULL,
            file_size_bytes INTEGER NOT NULL,
            file_hash TEXT NOT NULL,
            
            -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
            was_converted BOOLEAN NOT NULL,
            conversion_time_sec REAL,
            
            -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            model_name TEXT NOT NULL,
            device_type TEXT NOT NULL,
            
            -- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            max_tokens INTEGER,
            torch_dtype TEXT,
            
            -- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
            description_text TEXT NOT NULL,
            description_length INTEGER NOT NULL,
            
            -- –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            inference_time_sec REAL NOT NULL,
            generation_time_sec REAL NOT NULL,
            total_processing_time_sec REAL NOT NULL,
            
            -- –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
            image_width INTEGER,
            image_height INTEGER,
            
            -- –°—Ç–∞—Ç—É—Å
            status TEXT NOT NULL,
            error_message TEXT,
            
            -- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (JSON)
            metadata TEXT,
            
            -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            UNIQUE(file_hash, request_timestamp)
        )
    """)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_file_hash 
        ON inference_logs(file_hash)
    """)
    
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_timestamp 
        ON inference_logs(request_timestamp)
    """)
    
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_status 
        ON inference_logs(status)
    """)
    
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_model_device 
        ON inference_logs(model_name, device_type)
    """)
    
    conn.commit()
    conn.close()
    
    logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {DB_PATH}")


@contextmanager
def get_db_connection():
    """
    Context manager –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
    """
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # –î–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ –∫–æ–ª–æ–Ω–∫–∞–º –ø–æ –∏–º–µ–Ω–∏
    try:
        yield conn
        conn.commit()
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ë–î: {e}")
        raise
    finally:
        conn.close()


def log_inference_request(
    file_name: str,
    file_type: str,
    file_size: int,
    file_hash: str,
    was_converted: bool,
    conversion_time: Optional[float],
    model_name: str,
    device_type: str,
    description: str,
    inference_time: float,
    generation_time: float,
    total_time: float,
    image_size: Optional[tuple],
    max_tokens: Optional[int] = None,
    torch_dtype: Optional[str] = None,
    status: str = "success",
    error_message: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None
) -> int:
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            timestamp = datetime.utcnow().isoformat()
            description_length = len(description) if description else 0
            image_width = image_size[0] if image_size else None
            image_height = image_size[1] if image_size else None
            metadata_json = json.dumps(metadata) if metadata else None
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
            cursor.execute("""
                INSERT INTO inference_logs (
                    request_timestamp, file_name, file_type, file_size_bytes, file_hash,
                    was_converted, conversion_time_sec,
                    model_name, device_type,
                    max_tokens, torch_dtype,
                    description_text, description_length,
                    inference_time_sec, generation_time_sec, total_processing_time_sec,
                    image_width, image_height,
                    status, error_message, metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                timestamp, file_name, file_type, file_size, file_hash,
                was_converted, conversion_time,
                model_name, device_type,
                max_tokens, torch_dtype,
                description, description_length,
                inference_time, generation_time, total_time,
                image_width, image_height,
                status, error_message, metadata_json
            ))
            
            log_id = cursor.lastrowid
            
            logger.info(f"üìù –ó–∞–ø—Ä–æ—Å –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω –≤ –ë–î (ID: {log_id})")
            return log_id
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ –ë–î: {e}")
        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ä–∞–±–æ—Ç—É –µ—Å–ª–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
        return -1


def get_request_by_hash(file_hash: str) -> Optional[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–∫–∏–º –∂–µ —Ö—ç—à–µ–º —Ñ–∞–π–ª–∞ (–¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT * FROM inference_logs 
                WHERE file_hash = ? AND status = 'success'
                ORDER BY request_timestamp DESC 
                LIMIT 1
            """, (file_hash,))
            
            row = cursor.fetchone()
            
            if row:
                return dict(row)
            return None
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ —Ö—ç—à—É: {e}")
        return None


def get_statistics() -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∑–∞–ø—Ä–æ—Å–∞–º
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("""
                SELECT 
                    COUNT(*) as total_requests,
                    SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful,
                    SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as failed,
                    AVG(total_processing_time_sec) as avg_processing_time,
                    AVG(inference_time_sec) as avg_inference_time,
                    AVG(generation_time_sec) as avg_generation_time,
                    MIN(total_processing_time_sec) as min_processing_time,
                    MAX(total_processing_time_sec) as max_processing_time
                FROM inference_logs
            """)
            
            stats = dict(cursor.fetchone())
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º
            cursor.execute("""
                SELECT 
                    device_type,
                    COUNT(*) as count,
                    AVG(inference_time_sec) as avg_time
                FROM inference_logs
                WHERE status = 'success'
                GROUP BY device_type
            """)
            
            stats['by_device'] = [dict(row) for row in cursor.fetchall()]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º
            cursor.execute("""
                SELECT 
                    model_name,
                    COUNT(*) as count,
                    AVG(inference_time_sec) as avg_time
                FROM inference_logs
                WHERE status = 'success'
                GROUP BY model_name
            """)
            
            stats['by_model'] = [dict(row) for row in cursor.fetchall()]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
            cursor.execute("""
                SELECT 
                    file_type,
                    COUNT(*) as count,
                    SUM(CASE WHEN was_converted THEN 1 ELSE 0 END) as converted_count
                FROM inference_logs
                GROUP BY file_type
            """)
            
            stats['by_file_type'] = [dict(row) for row in cursor.fetchall()]
            
            return stats
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {}


def get_recent_requests(limit: int = 10) -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –∑–∞–ø—Ä–æ—Å–æ–≤
    
    Args:
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT 
                    id, request_timestamp, file_name, file_type,
                    model_name, device_type, status,
                    total_processing_time_sec, description_length
                FROM inference_logs
                ORDER BY request_timestamp DESC
                LIMIT ?
            """, (limit,))
            
            return [dict(row) for row in cursor.fetchall()]
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
        return []


def cleanup_old_records(days: int = 30):
    """
    –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–Ω–µ–π
    
    Args:
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            cutoff_date = datetime.utcnow().replace(
                hour=0, minute=0, second=0, microsecond=0
            )
            cutoff_date = cutoff_date.replace(day=cutoff_date.day - days)
            cutoff_str = cutoff_date.isoformat()
            
            cursor.execute("""
                DELETE FROM inference_logs
                WHERE request_timestamp < ?
            """, (cutoff_str,))
            
            deleted_count = cursor.rowcount
            
            logger.info(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ {deleted_count} —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π (—Å—Ç–∞—Ä—à–µ {days} –¥–Ω–µ–π)")
            
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π: {e}")