# validate.py
import time
import json
import os
import torch
import psutil
import pandas as pd
import Levenshtein
from tqdm import tqdm
from PIL import Image
from transformers import AutoProcessor
from peft import PeftModel

# –ü–æ–ø—ã—Ç–∫–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–Ω–µ —É–ø–∞–¥—ë—Ç, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç)
try:
    from transformers import Qwen3VLForConditionalGeneration as _QWEN3_CLASS
except Exception:
    _QWEN3_CLASS = None

try:
    from transformers import Qwen2VLForConditionalGeneration as _QWEN2_CLASS
except Exception:
    _QWEN2_CLASS = None

from qwen_vl_utils import process_vision_info

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
# –≠—Ç–∏ –±–∞–∑–æ–≤—ã–µ id –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏/—Ñ–æ–ª–ª–±—ç–∫–µ
QWEN2_BASE = "Qwen/Qwen2-VL-2B-Instruct"
QWEN3_BASE = "Qwen/Qwen3-VL-2B-Instruct"

# –ü–æ–º–µ–Ω—è–π—Ç–µ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
ADAPTER_PATH = "model_vlm_qwen3/weights"   # –∏–ª–∏ model_vlm_qwen3/weights
VAL_FILE = "data/val.jsonl"
IMAGES_DIR = "data/images"

# detect device: prefer mps, then cuda, then cpu
if torch.backends.mps.is_available():
    DEVICE = torch.device("mps")
elif torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")


def get_memory_usage():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –≤ –ú–ë"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def parse_markdown_table(text):
    """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ"""
    lines = text.split("\n")
    # –°—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å |, –Ω–æ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º --- –∏ –Ω–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
    data_lines = [l for l in lines if l.strip().startswith("|") and ("---" not in l) and ("–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ" not in l)]
    return len(data_lines)


def detect_adapter_variant(adapter_path: str) -> str:
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –¥–ª—è –∫–∞–∫–æ–π –º–æ–¥–µ–ª–∏ (qwen2 –∏–ª–∏ qwen3) —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∞–¥–∞–ø—Ç–µ—Ä—ã/processor.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'qwen3', 'qwen2' –∏–ª–∏ 'unknown'.
    """
    if not os.path.isdir(adapter_path):
        return "unknown"

    candidates = [
        "preprocessor_config.json",
        "adapter_config.json",  # PEFT
        "config.json",
        "tokenizer_config.json",
    ]

    for fname in candidates:
        p = os.path.join(adapter_path, fname)
        if not os.path.exists(p):
            continue
        try:
            with open(p, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception:
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ json –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ qwen3/qwen2
        def inspect_obj(o):
            if isinstance(o, str):
                s = o.lower()
                if "qwen3" in s or "qwen-3" in s:
                    return "qwen3"
                if "qwen2" in s or "qwen-2" in s:
                    return "qwen2"
            elif isinstance(o, dict):
                for v in o.values():
                    res = inspect_obj(v)
                    if res:
                        return res
            elif isinstance(o, list):
                for v in o:
                    res = inspect_obj(v)
                    if res:
                        return res
            return None

        res = inspect_obj(data)
        if res:
            return res

    # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏
    name = os.path.basename(os.path.normpath(adapter_path)).lower()
    if "qwen3" in name or "qwen_3" in name or "qwen-3" in name:
        return "qwen3"
    if "qwen2" in name or "qwen_2" in name or "qwen-2" in name:
        return "qwen2"

    return "unknown"


def choose_model_class(variant: str):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç tuple (model_class, base_model_id).
    –ü—ã—Ç–∞–µ—Ç—Å—è –≤—ã–±—Ä–∞—Ç—å Qwen3 —Å–Ω–∞—á–∞–ª–∞, –∑–∞—Ç–µ–º Qwen2.
    """
    if variant == "qwen3":
        if _QWEN3_CLASS is not None:
            return _QWEN3_CLASS, QWEN3_BASE
        # –µ—Å–ª–∏ –∫–ª–∞—Å—Å –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω, –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–æ–ø—Ä–æ–±—É–µ–º QWEN3_BASE –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        return None, QWEN3_BASE
    if variant == "qwen2":
        if _QWEN2_CLASS is not None:
            return _QWEN2_CLASS, QWEN2_BASE
        return None, QWEN2_BASE
    # unknown: –ø—Ä–µ–¥–ø–æ—á–µ—Å—Ç—å Qwen3 –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ Qwen2
    if _QWEN3_CLASS is not None:
        return _QWEN3_CLASS, QWEN3_BASE
    if _QWEN2_CLASS is not None:
        return _QWEN2_CLASS, QWEN2_BASE
    # –æ–±–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî –≤–µ—Ä–Ω—ë–º None, –∏ –±—É–¥–µ–º –ø—ã—Ç–∞—Ç—å—Å—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏
    return None, QWEN3_BASE


def validate():
    print("=== –ó–ê–ü–£–°–ö –í–ê–õ–ò–î–ê–¶–ò–ò ===")
    print("ADAPTER_PATH:", ADAPTER_PATH)
    variant = detect_adapter_variant(ADAPTER_PATH)
    print("Detected adapter variant:", variant)

    model_class, base_model_id = choose_model_class(variant)
    print("Chosen base model id:", base_model_id)
    mem_start = get_memory_usage()

    # –ü–æ–¥–±–æ—Ä dtype –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if DEVICE.type == "mps":
        torch_dtype = torch.float16
        use_device_map = False
    elif DEVICE.type == "cuda":
        torch_dtype = torch.bfloat16
        use_device_map = True
    else:
        torch_dtype = torch.float32
        use_device_map = False

    model = None
    processor = None

    # 1) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å —Å variant-first —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π:
    load_errors = []
    tried_classes = []
    # —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: –µ—Å–ª–∏ detect –≤–µ—Ä–Ω—É–ª qwen3/qwen2, —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π,
    # –∑–∞—Ç–µ–º –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–æ–π (—Ñ–æ–ª–ª–±—ç–∫)
    candidates = []
    if variant == "qwen3":
        candidates = [("qwen3", _QWEN3_CLASS, QWEN3_BASE), ("qwen2", _QWEN2_CLASS, QWEN2_BASE)]
    elif variant == "qwen2":
        candidates = [("qwen2", _QWEN2_CLASS, QWEN2_BASE), ("qwen3", _QWEN3_CLASS, QWEN3_BASE)]
    else:
        candidates = [("qwen3", _QWEN3_CLASS, QWEN3_BASE), ("qwen2", _QWEN2_CLASS, QWEN2_BASE)]

    for name, cls, base in candidates:
        try:
            print(f"–ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {name} (base id {base}) ...")
            if cls is not None:
                # –ö–ª–∞—Å—Å –¥–æ—Å—Ç—É–ø–µ–Ω –≤ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–µ
                if use_device_map and hasattr(torch, "device") and DEVICE.type == "cuda":
                    model = cls.from_pretrained(base, torch_dtype=torch_dtype, device_map="auto")
                else:
                    model = cls.from_pretrained(base, torch_dtype=torch_dtype)
                    model.to(DEVICE)
            else:
                # –µ—Å–ª–∏ –∫–ª–∞—Å—Å –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω, –ø—Ä–æ–±—É–µ–º generic from_pretrained —á–µ—Ä–µ–∑ AutoModel (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π)
                # –∑–¥–µ—Å—å –≤—Å—ë —Ä–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º base id, transformers –ø–æ–¥–±–µ—Ä—ë—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
                from transformers import AutoModelForCausalLM
                if use_device_map and DEVICE.type == "cuda":
                    model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch_dtype, device_map="auto")
                else:
                    model = AutoModelForCausalLM.from_pretrained(base, torch_dtype=torch_dtype)
                    model.to(DEVICE)
            print(f"–ú–æ–¥–µ–ª—å {name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
            model_base_used = base
            break
        except Exception as e:
            err = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {name}: {e}"
            load_errors.append(err)
            tried_classes.append(name)
            print(err)
            model = None

    if model is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ Qwen3, –Ω–∏ Qwen2 —á–µ—Ä–µ–∑ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã. –í—ã–≤–æ–¥–∏–º –æ—à–∏–±–∫–∏ –∏ –≤—ã—Ö–æ–¥–∏–º.")
        for e in load_errors:
            print(e)
        return

    # 2) –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä ‚Äî –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º ADAPTER_PATH
    try:
        processor = AutoProcessor.from_pretrained(ADAPTER_PATH)
        print("Processor –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ ADAPTER_PATH:", ADAPTER_PATH)
    except Exception:
        try:
            processor = AutoProcessor.from_pretrained(model_base_used, min_pixels=256*28*28, max_pixels=512*28*28)
            print("Processor –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ base model:", model_base_used)
        except Exception as e:
            print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å processor:", e)
            return

    # 3) –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (PEFT)
    try:
        model = PeftModel.from_pretrained(model, ADAPTER_PATH)
        print("‚úÖ –ê–¥–∞–ø—Ç–µ—Ä—ã LoRA –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑:", ADAPTER_PATH)
    except Exception as e:
        print("‚ö†Ô∏è –ê–¥–∞–ø—Ç–µ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã/–Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å. –í–∞–ª–∏–¥–∏—Ä—É–µ–º –Ω–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –û—à–∏–±–∫–∞:", e)

    mem_model_loaded = get_memory_usage()
    print(f"–ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {mem_model_loaded:.1f} MB (+{mem_model_loaded - mem_start:.1f} MB)")

    # 4) –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    with open(VAL_FILE, "r", encoding="utf-8") as f:
        val_data = [json.loads(line) for line in f]
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ {len(val_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")

    metrics = {"latency": [], "text_similarity": [], "step_count_diff": []}

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π —Å—Ç—Ä–æ–≥–∏–π prompt –∏ add_generation_prompt=False
    prompt = (
        "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ BPMN. –í—ã–¥–∞–≤–∞–π –æ—Ç–≤–µ—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown-—Ç–∞–±–ª–∏—Ü—ã. "
        "–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ—á–Ω–æ: | ‚Ññ | –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è | –†–æ–ª—å |."
    )

    model.eval()

    for item in tqdm(val_data):
        image_path = os.path.join(IMAGES_DIR, item["file_name"])
        ground_truth = item["text"]

        image = Image.open(image_path).convert("RGB")
        messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]

        # —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤—Ö–æ–¥–∞ —Å add_generation_prompt=False (—á—Ç–æ–±—ã —Å–æ–≤–ø–∞–¥–∞–ª–æ —Å training)
        if hasattr(processor, "apply_chat_template"):
            text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        else:
            text_input = prompt

        image_inputs, video_inputs = process_vision_info(messages)
        proc_kwargs = {"text": [text_input], "padding": True, "return_tensors": "pt"}
        if image_inputs is not None:
            proc_kwargs["images"] = image_inputs
        if video_inputs is not None:
            proc_kwargs["videos"] = video_inputs

        inputs = processor(**proc_kwargs)

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Ö–æ–¥—ã –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤ device_map)
        try:
            inputs = inputs.to(model.device)
        except Exception:
            # –ï—Å–ª–∏ model.device –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, device_map="auto"), –ø–µ—Ä–µ–Ω–µ—Å—ë–º –Ω–∞ DEVICE –≤—Ä—É—á–Ω—É—é
            inputs = {k: (v.to(DEVICE) if hasattr(v, "to") else v) for k, v in inputs.items()}

        start_time = time.perf_counter()
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=False)
        end_time = time.perf_counter()

        latency = end_time - start_time
        metrics["latency"].append(latency)

        # –¢—Ä–∏–º–º–∏–º —Å —É—á—ë—Ç–æ–º –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        # –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö inputs.input_ids –º–æ–∂–µ—Ç –±—ã—Ç—å tensor –∏–ª–∏ attribute access style
        in_ids = inputs["input_ids"] if isinstance(inputs, dict) else inputs.input_ids
        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(in_ids, generated_ids)]

        prediction = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]

        sim = Levenshtein.ratio(prediction, ground_truth)
        metrics["text_similarity"].append(sim)

        gt_steps = parse_markdown_table(ground_truth)
        pred_steps = parse_markdown_table(prediction)
        metrics["step_count_diff"].append(abs(gt_steps - pred_steps))

    # 5) –û—Ç—á–µ—Ç
    df = pd.DataFrame(metrics)
    print("\n" + "=" * 30)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò")
    print("=" * 30)
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (Latency): {df['latency'].mean():.2f} —Å–µ–∫/img")
    print(f"–ú–∞–∫—Å –≤—Ä–µ–º—è:              {df['latency'].max():.2f} —Å–µ–∫/img")
    print(f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ (<20 —Å–µ–∫):    {'‚úÖ PASS' if df['latency'].max() < 20 else '‚ùå FAIL'}")
    print("-" * 30)
    print(f"–°—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ (Sim):   {df['text_similarity'].mean():.2%}")
    print(f"–û—à–∏–±–∫–∏ –≤ –∫–æ–ª-–≤–µ —à–∞–≥–æ–≤:   {df['step_count_diff'].mean():.2f}")
    print("-" * 30)
    print(f"–ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM:         {mem_model_loaded:.1f} MB")
    print(f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ (<8 GB):      {'‚úÖ PASS' if mem_model_loaded < 8192 else '‚ùå FAIL'}")
    print("=" * 30)

    df.to_csv("validation_results.csv", index=False)
    print("–î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ validation_results.csv")


if __name__ == "__main__":
    validate()
