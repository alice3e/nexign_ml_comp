import os
import torch
import streamlit as st
from PIL import Image
# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å –¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π Qwen (2.5 –∏ 3)
try:
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
except ImportError:
    # –ï—Å–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Å—Ç–∞—Ä–∞—è, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Å, –Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å –¥–ª—è Qwen3
    from transformers import Qwen2VLForConditionalGeneration as Qwen2_5_VLForConditionalGeneration
    from transformers import AutoProcessor

from peft import PeftModel
from qwen_vl_utils import process_vision_info

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
# ID –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –ø—Ä–æ—Å–∏–ª–∞
BASE_MODEL_ID = "Qwen/Qwen3-VL-2B-Instruct"

# –ü—É—Ç—å –∫ –≤–µ—Å–∞–º (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–ø–∫—É model_vlm_qwen3 –¥–ª—è –ø–æ—Ä—è–¥–∫–∞)
ADAPTER_PATH = os.getenv("MODEL_PATH", os.path.join("model_vlm_qwen3", "weights"))

# === –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===
@st.cache_resource
def load_model_and_processor():
    st.toast(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {BASE_MODEL_ID}...", icon="‚è≥")
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ {BASE_MODEL_ID} –∏–∑: {ADAPTER_PATH}")
    
    # 1. –ì—Ä—É–∑–∏–º –±–∞–∑—É
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º bfloat16 –¥–ª—è Mac/MPS
        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            BASE_MODEL_ID,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
    except OSError:
        st.error(f"‚ùå –ú–æ–¥–µ–ª—å '{BASE_MODEL_ID}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ HuggingFace. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–ª–∏ –¥–æ—Å—Ç—É–ø.")
        st.stop()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        st.stop()
    
    # 2. –ì—Ä—É–∑–∏–º –∞–¥–∞–ø—Ç–µ—Ä—ã (LoRA)
    if os.path.exists(ADAPTER_PATH):
        try:
            model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
            print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω—ã.")
            st.toast("LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω—ã!", icon="‚úÖ")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ LoRA: {e}")
            model = base_model
    else:
        st.warning(f"‚ö†Ô∏è –í–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {ADAPTER_PATH}. –†–∞–±–æ—Ç–∞–µ–º –Ω–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏.")
        model = base_model

    # 3. –ì—Ä—É–∑–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    try:
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–µ—Å–ª–∏ —Å–æ—Ö—Ä–∞–Ω—è–ª–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        processor = AutoProcessor.from_pretrained(ADAPTER_PATH, min_pixels=256*28*28, max_pixels=512*28*28)
    except:
        # –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –∫–∞—á–∞–µ–º –∏–∑ —Ö–∞–±–∞
        processor = AutoProcessor.from_pretrained(BASE_MODEL_ID, min_pixels=256*28*28, max_pixels=512*28*28)
        
    return model, processor

# === –ò–ù–¢–ï–†–§–ï–ô–° ===
st.set_page_config(page_title="Qwen3 BPMN", page_icon="üîÆ", layout="centered")

st.title("üîÆ BPMN Reader (Qwen3-VL)")
st.caption(f"Model ID: `{BASE_MODEL_ID}`")

# –°–∞–π–¥–±–∞—Ä
with st.sidebar:
    st.header("–°—Ç–∞—Ç—É—Å")
    if os.path.exists(ADAPTER_PATH):
        st.success(f"Fine-tuned weights detected")
    else:
        st.warning("Base model mode")
    
    st.markdown("---")
    st.markdown("**–ù–∞—Å—Ç—Ä–æ–π–∫–∏:**")
    # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–∑—É–Ω–æ–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    temperature = st.slider("Temperature", 0.0, 1.0, 0.1)

# –ó–∞–≥—Ä—É–∑–∫–∞
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∏–∞–≥—Ä–∞–º–º—É", type=["png", "jpg", "jpeg"])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="–í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", use_column_width=True)
    
    if st.button("‚ú® –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ", type="primary"):
        with st.spinner("–ê–Ω–∞–ª–∏–∑ –¥–∏–∞–≥—Ä–∞–º–º—ã..."):
            # –ó–∞–≥—Ä—É–∑–∫–∞ (–æ–¥–∏–Ω —Ä–∞–∑)
            model, processor = load_model_and_processor()
            
            # –ü—Ä–æ–º–ø—Ç
            prompt = "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ BPMN. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–≥—Ä–∞–º–º—É –∏ —Å–æ–∑–¥–∞–π Markdown —Ç–∞–±–ª–∏—Ü—É —Å —à–∞–≥–∞–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞."
            
            messages = [{"role": "user", "content": [{"type": "image", "image": image}, {"type": "text", "text": prompt}]}]
            
            text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            
            inputs = processor(
                text=[text_input], images=image_inputs, videos=video_inputs,
                padding=True, return_tensors="pt"
            ).to(model.device)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs, 
                    max_new_tokens=1024, 
                    do_sample=False if temperature == 0 else True,
                    temperature=temperature if temperature > 0 else None
                )

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            output_text = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

            st.markdown("### –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            st.markdown(output_text)